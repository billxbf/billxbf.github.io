<!doctype html><html lang=en dir=auto data-theme=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Demystifying Agent Sandbox | B'Log</title><meta name=keywords content><meta name=description content="Modern AI agents are typically scaffolded with a runtime sandbox, and these Computer-Use Agents (CUA) autonomously run code, use the terminal, take notes, and access the Internet and MCPs &ndash; exactly like humans do when interacting with the digital world.
Yet the underlying reasoning and practices remain unclear to most, so let&rsquo;s dive into popular agent scaffolds like Claude Code and MiniMax Agent, demystifying the design principles and discovering how agents benefit from using a computer."><meta name=author content><link rel=canonical href=http://localhost:1313/posts/demystify-agent-sandbox/><link crossorigin=anonymous href=/assets/css/stylesheet.343cc480b9ffc8f04ccbe5e968ad674880cab773ec19905e93033065c1e7a804.css integrity="sha256-NDzEgLn/yPBMy+XpaK1nSIDKt3PsGZBekwMwZcHnqAQ=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon.ico><link rel=apple-touch-icon href=http://localhost:1313/favicon.ico><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/demystify-agent-sandbox/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51);color-scheme:dark}.list{background:var(--theme)}.toc{background:var(--entry)}}@media(prefers-color-scheme:light){.list::-webkit-scrollbar-thumb{border-color:var(--code-bg)}}</style></noscript><script>localStorage.getItem("pref-theme")==="dark"?document.querySelector("html").dataset.theme="dark":localStorage.getItem("pref-theme")==="light"?document.querySelector("html").dataset.theme="light":window.matchMedia("(prefers-color-scheme: dark)").matches?document.querySelector("html").dataset.theme="dark":document.querySelector("html").dataset.theme="light"</script><meta property="og:url" content="http://localhost:1313/posts/demystify-agent-sandbox/"><meta property="og:site_name" content="B'Log"><meta property="og:title" content="Demystifying Agent Sandbox"><meta property="og:description" content="Modern AI agents are typically scaffolded with a runtime sandbox, and these Computer-Use Agents (CUA) autonomously run code, use the terminal, take notes, and access the Internet and MCPs – exactly like humans do when interacting with the digital world.
Yet the underlying reasoning and practices remain unclear to most, so let’s dive into popular agent scaffolds like Claude Code and MiniMax Agent, demystifying the design principles and discovering how agents benefit from using a computer."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-12-28T00:00:00+00:00"><meta property="article:modified_time" content="2025-12-28T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Demystifying Agent Sandbox"><meta name=twitter:description content="Modern AI agents are typically scaffolded with a runtime sandbox, and these Computer-Use Agents (CUA) autonomously run code, use the terminal, take notes, and access the Internet and MCPs &ndash; exactly like humans do when interacting with the digital world.
Yet the underlying reasoning and practices remain unclear to most, so let&rsquo;s dive into popular agent scaffolds like Claude Code and MiniMax Agent, demystifying the design principles and discovering how agents benefit from using a computer."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Demystifying Agent Sandbox","item":"http://localhost:1313/posts/demystify-agent-sandbox/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Demystifying Agent Sandbox","name":"Demystifying Agent Sandbox","description":"Modern AI agents are typically scaffolded with a runtime sandbox, and these Computer-Use Agents (CUA) autonomously run code, use the terminal, take notes, and access the Internet and MCPs \u0026ndash; exactly like humans do when interacting with the digital world.\nYet the underlying reasoning and practices remain unclear to most, so let\u0026rsquo;s dive into popular agent scaffolds like Claude Code and MiniMax Agent, demystifying the design principles and discovering how agents benefit from using a computer.\n","keywords":[],"articleBody":"Modern AI agents are typically scaffolded with a runtime sandbox, and these Computer-Use Agents (CUA) autonomously run code, use the terminal, take notes, and access the Internet and MCPs – exactly like humans do when interacting with the digital world.\nYet the underlying reasoning and practices remain unclear to most, so let’s dive into popular agent scaffolds like Claude Code and MiniMax Agent, demystifying the design principles and discovering how agents benefit from using a computer.\nWhy Sandbox? In short, Context Delegation and Runtime Isolation.\nContext Delegation. Even with memory-efficient tricks like KV caching and Linear Attention architectures, long-context reasoning is still a nontrivial challenge to AI agents. Production AI agents are usually packed with bloated tool descriptions and system prompts to cover more case handling and example following. Context length can easily grow to 100k or over 1M in multi-turn agentic trajectories – especially when tool responses contain extraneous data. Growing contexts dilute attention and cause huge memory burden to both training and inference.\nIntuitively, you’d want to move episodic context out of the main agent loop using:\nSubagent: Spin up subtasks to invoke another agent. Without sharing a full context, subagents can easily duplicate work, wasting excessive tokens, while over-engineered orchestration introduces inductive bias. Filesystem: Use the filesystem to keep todo lists, agent memory, and conditional instructions (aka Agent Skill) that the agent chooses to load into the context when necessary. Sandbox Filesystem is an agent’s extended context through computer-use. Runtime Isolation. Sandboxing keeps all agent actions in a tightly controlled environment, protecting the host system by isolating potential errors and de-risking real user data or resources. That isolation is especially critical when executing popular and tools.\nCursor (Agent mode) running commands in a secure sandbox\nClaude Code – More than just Code Claude Code has gained huge traction in 2025. It was originally built for coding assistance, but Anthropic is clearly steering it toward more general agent use cases. As highlighted in recent podcasts, Claude Code already excels at, or can be extended to, deep research and vertical specialist agents. Andrej has also been tweeting mini projects like agentic home control in the physical world.\nClaude Code scaffolding. The agent controls a virtual sandbox with Bash and coding. Context delegated to filesystem (Skills). Tools executed outside the sandbox are implemented as MCP.\nAgentic Computer Use What’s the magic here? Metaphorically, an AI Agent equipped with a Filesystem and Runtime Environment is just a human using a computer – the primary, if not only interface connecting humans to the Digital World. Imagine the action space. ¯_(ツ)_/¯ With that lens, scaffolding an agent turns into OS design:\nConfiguring a Linux Docker / VM, or even creating a completely native OS from scratch (yes, there are a few working on it). Designing the Filesystem within the sandbox. Pre-install some “Apps” for the agents – a Terminal, a Browser, a writing pad … and place some instructions for the agent to refer to before use (Anthropic names it SKILL.md). Meanwhile, give agents a “shortcut” to invoke outside endpoints – like credential-related database querying. These fit into the bucket of traditional function-calling or MCP. Claude Code System Prompt Anthropic still doesn’t publicly share Claude Code’s system prompt and tool implementations in its Claude Agent SDK. However, there’s an interesting thread where people trace and hack Claude Code’s request \u0026 response, and reverse engineer the system prompt and its commit diff across versions. From that prompt, we can clearly find some sparks and lessons in agent tool implementations.\nFor example, Claude Code doesn’t use an interactive browser like browser-use does by default. Instead it splits capabilities into separate WebSearch and WebFetch tools, likely due to speed \u0026 efficiency concerns.\nSummary of Claude Code’s tools from the hacked system prompt.\nClaude Code Filesystem When locally deployed, Claude Code uses ~/.claude/ for its own sandbox workspace, and it mounts your work project under ~/.claude/projects/. The scaffold keeps plugins (MCP integrations) and skills in different directories, and also tracks TODO, personalization configs, and metadata like command history and debug logs.\nClaude Code’s working filesystem.\nMiniMax Agent MiniMax has been the most surprising AI lab for me this year. It ships the #1 (as of Dec 2025) OSS model on LMArena WebDev at 229B weights, which is considered “small” among parallel flagship models. Meanwhile, the agent scaffold, MiniMax Agent creates surprisingly good apps and reports with prolonged reasoning and autonomous execution. Here’s my favorite trajectory replay.\nMiniMax Agent using browser for autonomous app creation (Netflix Clone)\nMiniMax Agent has a UI component that lets you navigate the agent’s sandbox filesystem in realtime, and you can prompt the agent to summarize its initial filesystem state. The workspace is a Python-based development environment designed for AI agents with integrated external API access capabilities. Rich modular data sources connect to various third-party APIs including Twitter, Yahoo Finance, TripAdvisor, Pinterest, Patents, Scholar, Commodities, Metals, and Booking.com. None of these live inside the system prompt, which lets the agent fully leverage context delegation in the sandbox.\nMiniMax Agent’s working filesystem.\nOther Computer-use Agents Major AI labs have all been building Computer-use Agents, each guided by its own product philosophy. OpenAI seems to have made great progress on Operator and ChatGPT Atlas. I really enjoy the visual presentation and dynamics of the agent sandbox at runtime.\nOpenAI Agent using Terminal\nWhen you prompt Operator to describe its filesystem, you’ll observe a complete Linux VM with two working directories – /home/oai containing session data and /openai storing internal “Skills”. From ChatGPT’s self manifest, the only skill installed is a Browser.\nOpenAI Agent filesystem\nBeyond OpenAI, numerous teams are pushing here as well. For example, Google AI Studio (Build) can build and test apps from ideas with advanced multimodal capabilities from Gemini, and Manus orchestrates a huge number of subagents and services like browser-use to max out agent action space. I’ll leave the rest of the exploration to readers due to limited time.\nAgentic RL Agent, Action (tools) and Environment (scaffold) are three tightly bound concepts in the traditional literature. Consistency between training (rollout) Gym and inference scaffold is critical to maintain agent performance. However, this consistency has become a luxury since model providers won’t expose training infra, usually resulting in tedious engineering efforts guessing the “right” scaffolding and orchestrations among downstream agent builders.\nMost LLMs are trained with vanilla sandbox scaffolds to support evaluation like Terminal-Bench, yet a language model doesn’t natively “know” how to use your sandbox – especially when you want to customize the tools and “Skills”. In this case, RL becomes an effective data-efficient method for your last mile.\nRollout Infra One key challenge in Agentic RL is to build stable and efficient rollout infra. The additional factor of sandbox and tools like browser impose difficulty in asynchronous runtime efficiency, state management, and security. These rollouts are usually magnitudes more expensive (time and effort) than non-agentic RL like Math CoT. A nice starting point is OpenHands V1 released lately. The architecture of decoupled modules (abstraction, tools, sandbox, and server) provides solid coordination with reusable modules across scaffolding and rollout serving. Besides, Pytorch OpenEnv provides a nice Gymnasium-style endpoint over commonly used agent docker environments.\nOpenHands V1 with decoupled modules reusable across rollout and scaffolding.\nReward Design and Training Recipe Another challenge in Agentic RL is defining the reward function. Computer-use agents usually tackle open-ended tasks like research and app building, which lack unbiased and verifiable scoring mechanisms as in Math and Coding. Meanwhile, vanilla use of LLM-as-judge to generate rewards can easily trap the policy into adversarial distribution from the teacher (Reward Hacking). This is also confirmed in Andrej’s recent interview “RL is terrible”. An interesting approach to mitigate reward hacking in open-ended research is brought by AI2 DR Tulu, where rubrics are buffered and generated on the fly together with policy. The algorithm setups used for Agentic RL generally follow the lessons from long-context RL in LLMs, such as broadening exploration in prolonged steps.\nReinforcement Learning with Evolving Rubrics (RLER)\nConclusion This post showcases the state of Computer-use Agents – from product to design principles. It elaborates the reasoning and importance of runtime sandbox in AI Agents and briefly introduces the engineering and training challenges. 2026 will be the year of Computer-use Agents – with focus shifting from Prompt Engineering to Sandbox Engineering and RL on custom scaffolds ;)\nCitation Xu, Binfeng. \"Demystifying Agent Sandbox\". B'Log (Dec 2025). https://billxbf.github.io/posts/demystify-agent-sandbox/ ","wordCount":"1401","inLanguage":"en","datePublished":"2025-12-28T00:00:00Z","dateModified":"2025-12-28T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/demystify-agent-sandbox/"},"publisher":{"@type":"Organization","name":"B'Log","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="B'Log (Alt + H)">B'Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Demystifying Agent Sandbox</h1><div class=post-meta><span title='2025-12-28 00:00:00 +0000 UTC'>December 28, 2025</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#why-sandbox>Why Sandbox?</a></li><li><a href=#claude-code--more-than-just-code>Claude Code &ndash; More than just Code</a><ul><li><a href=#agentic-computer-use>Agentic Computer Use</a></li><li><a href=#claude-code-system-prompt>Claude Code System Prompt</a></li><li><a href=#claude-code-filesystem>Claude Code Filesystem</a></li></ul></li><li><a href=#minimax-agent>MiniMax Agent</a></li><li><a href=#other-computer-use-agents>Other Computer-use Agents</a></li><li><a href=#agentic-rl>Agentic RL</a><ul><li><a href=#rollout-infra>Rollout Infra</a></li><li><a href=#reward-design-and-training-recipe>Reward Design and Training Recipe</a></li></ul></li><li><a href=#conclusion>Conclusion</a><ul><li><a href=#citation>Citation</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>Modern AI agents are typically scaffolded with a runtime sandbox, and these Computer-Use Agents (CUA) autonomously run code, use the terminal, take notes, and access the Internet and MCPs &ndash; exactly like humans do when interacting with the digital world.</p><p>Yet the underlying reasoning and practices remain unclear to most, so let&rsquo;s dive into popular agent scaffolds like <a href=https://www.anthropic.com/engineering/claude-code-best-practices>Claude Code</a> and <a href=https://agent.minimax.io/>MiniMax Agent</a>, demystifying the design principles and discovering how agents benefit from using a computer.</p><h2 id=why-sandbox>Why Sandbox?<a hidden class=anchor aria-hidden=true href=#why-sandbox>#</a></h2><p>In short, <strong>Context Delegation</strong> and <strong>Runtime Isolation</strong>.</p><p><strong>Context Delegation.</strong> Even with memory-efficient tricks like KV caching and Linear Attention architectures, long-context reasoning is still a nontrivial challenge to AI agents.
Production AI agents are usually packed with bloated tool descriptions and system prompts to cover more case handling and example following. Context length can easily grow to 100k or over 1M in multi-turn agentic trajectories &ndash; especially when tool responses contain extraneous data. Growing contexts dilute attention and cause huge memory burden to both training and inference.</p><p>Intuitively, you&rsquo;d want to move episodic context out of the main agent loop using:</p><ul><li><em><strong>Subagent</strong></em>: Spin up subtasks to invoke another agent. Without sharing a full context, subagents can easily duplicate work, wasting excessive tokens, while over-engineered orchestration introduces inductive bias.</li><li><em><strong>Filesystem</strong></em>: Use the filesystem to keep todo lists, agent memory, and conditional instructions (aka <a href=https://platform.claude.com/docs/en/agents-and-tools/agent-skills/overview>Agent Skill</a>) that the agent chooses to load into the context when necessary. <strong>Sandbox Filesystem is an agent&rsquo;s extended context through computer-use</strong>.</li></ul><p><strong>Runtime Isolation.</strong> Sandboxing keeps all agent actions in a tightly controlled environment, protecting the host system by isolating potential errors and de-risking real user data or resources.
That isolation is especially critical when executing popular <code>&lt;python></code> and <code>&lt;browser></code> tools.</p><figure class=align-center><img loading=lazy src=./assets/cursor-sandbox.png#center alt="Agent cursor in sandbox" width=350><figcaption><p><em>Cursor (Agent mode) running commands in a secure sandbox</em></p></figcaption></figure><h2 id=claude-code--more-than-just-code>Claude Code &ndash; More than just Code<a hidden class=anchor aria-hidden=true href=#claude-code--more-than-just-code>#</a></h2><p>Claude Code has gained huge traction in 2025. It was originally built for coding assistance, but Anthropic is clearly steering it toward more general agent use cases. As highlighted in <a href="https://www.youtube.com/watch?v=CEvIs9y1uog">recent podcasts</a>, Claude Code already excels at, or can be extended to, deep research and vertical specialist agents.
Andrej has also been <a href="https://x.com/karpathy/status/2005421816110862601?s=46&amp;t=muzAYwVgphxc-B1ajhy0EA">tweeting</a> mini projects like agentic home control in the <a href="https://x.com/karpathy/status/2005067301511630926?s=20">physical world</a>.</p><figure class=align-center><img loading=lazy src=./assets/claude_skill_computer.png#center alt="Claude skill" width=700><figcaption><p><em>Claude Code scaffolding. The agent controls a virtual sandbox with Bash and coding. Context delegated to filesystem (Skills). Tools executed outside the sandbox are implemented as MCP.</em></p></figcaption></figure><h3 id=agentic-computer-use>Agentic Computer Use<a hidden class=anchor aria-hidden=true href=#agentic-computer-use>#</a></h3><p>What&rsquo;s the magic here? Metaphorically, an AI Agent equipped with a Filesystem and Runtime Environment is just a human using a computer &ndash; the primary, if not only interface connecting humans to the Digital World. <strong>Imagine the action space.</strong> ¯_(ツ)_/¯ With that lens, scaffolding an agent turns into OS design:</p><ul><li>Configuring a Linux Docker / VM, or even creating a completely native OS from scratch (yes, there are a few working on it).</li><li>Designing the Filesystem within the sandbox.<ul><li>Pre-install some &ldquo;Apps&rdquo; for the agents &ndash; a Terminal, a Browser, a writing pad &mldr; and place some instructions for the agent to refer to before use (Anthropic names it <code>SKILL.md</code>).</li><li>Meanwhile, give agents a &ldquo;shortcut&rdquo; to invoke outside endpoints &ndash; like credential-related database querying. These fit into the bucket of traditional function-calling or MCP.</li></ul></li></ul><h3 id=claude-code-system-prompt>Claude Code System Prompt<a hidden class=anchor aria-hidden=true href=#claude-code-system-prompt>#</a></h3><p>Anthropic still doesn&rsquo;t publicly share Claude Code&rsquo;s system prompt and tool implementations in its <a href=https://platform.claude.com/docs/en/agent-sdk/overview>Claude Agent SDK</a>.
However, there&rsquo;s an interesting thread where people <a href=https://mariozechner.at/posts/2025-08-03-cchistory/#toc_0>trace and hack</a> Claude Code&rsquo;s request & response, and reverse engineer the system prompt and its commit <a href=https://cchistory.mariozechner.at/>diff</a> across versions. From that prompt, we can clearly find some sparks and lessons in agent tool implementations.</p><p>For example, Claude Code doesn&rsquo;t use an interactive browser like <a href=https://browser-use.com/>browser-use</a> does by default. Instead it splits capabilities into separate <code>WebSearch</code> and <code>WebFetch</code> tools, likely due to speed & efficiency concerns.</p><figure class=align-center><img loading=lazy src=./assets/claude_tools.png#center alt="Claude tools" width=700><figcaption><p><em>Summary of Claude Code&rsquo;s tools from the hacked system prompt.</em></p></figcaption></figure><h3 id=claude-code-filesystem>Claude Code Filesystem<a hidden class=anchor aria-hidden=true href=#claude-code-filesystem>#</a></h3><p>When locally deployed, Claude Code uses <code>~/.claude/</code> for its own sandbox workspace, and it mounts your work project under <code>~/.claude/projects/</code>.
The scaffold keeps <code>plugins</code> (MCP integrations) and <code>skills</code> in different directories, and also tracks TODO, personalization configs, and metadata like command history and debug logs.</p><figure class=align-center><img loading=lazy src=./assets/claude_code_fs.png#center alt="Claude Code filesystem" width=500><figcaption><p><em>Claude Code&rsquo;s working filesystem.</em></p></figcaption></figure><h2 id=minimax-agent>MiniMax Agent<a hidden class=anchor aria-hidden=true href=#minimax-agent>#</a></h2><p>MiniMax has been the most surprising AI lab for me this year. It ships the #1 (as of Dec 2025) OSS model on <a href=https://lmarena.ai/leaderboard/webdev>LMArena WebDev</a> at 229B weights, which is considered &ldquo;small&rdquo; among parallel flagship models. Meanwhile, the agent scaffold, <a href=https://agent.minimax.io/>MiniMax Agent</a> creates surprisingly good apps and reports with prolonged reasoning and autonomous execution. Here&rsquo;s my favorite <a href=https://agent.minimax.io/share/296564788117720>trajectory replay</a>.</p><figure class=align-center><img loading=lazy src=./assets/minimax_browser.png#center alt="MiniMax Browser Agent" width=500><figcaption><p><em>MiniMax Agent using browser for autonomous app creation (Netflix Clone)</em></p></figcaption></figure><p>MiniMax Agent has a UI component that lets you navigate the agent&rsquo;s sandbox filesystem in realtime, and you can prompt the agent to summarize its initial filesystem state.
The workspace is a Python-based development environment designed for AI agents with integrated external API access capabilities. Rich modular data sources connect to various third-party APIs including Twitter, Yahoo Finance, TripAdvisor, Pinterest, Patents, Scholar, Commodities, Metals, and Booking.com. None of these live inside the system prompt, which lets the agent fully leverage context delegation in the sandbox.</p><figure class=align-center><img loading=lazy src=./assets/minimax_fs.png#center alt="MiniMax Agent filesystem" width=500><figcaption><p><em>MiniMax Agent&rsquo;s working filesystem.</em></p></figcaption></figure><h2 id=other-computer-use-agents>Other Computer-use Agents<a hidden class=anchor aria-hidden=true href=#other-computer-use-agents>#</a></h2><p>Major AI labs have all been building Computer-use Agents, each guided by its own product philosophy. OpenAI seems to have made great progress on <a href=https://openai.com/index/introducing-operator/>Operator</a> and <a href=https://chatgpt.com/atlas>ChatGPT Atlas</a>. I really enjoy the visual presentation and dynamics of the agent sandbox at runtime.</p><figure class=align-center><img loading=lazy src=./assets/oai_cua.png#center alt="OpenAI Agent using Terminal" width=500><figcaption><p><em>OpenAI Agent using Terminal</em></p></figcaption></figure><p>When you prompt Operator to describe its filesystem, you&rsquo;ll observe a complete Linux VM with two working directories &ndash; <code>/home/oai</code> containing session data and <code>/openai</code> storing internal &ldquo;Skills&rdquo;.
From ChatGPT&rsquo;s self manifest, the only skill installed is a Browser.</p><figure class=align-center><img loading=lazy src=./assets/oai_fs.png#center alt="OpenAI Agent filesystem" width=500><figcaption><p><em>OpenAI Agent filesystem</em></p></figcaption></figure><p>Beyond OpenAI, numerous teams are pushing here as well. For example, <a href=https://aistudio.google.com/u/1/apps>Google AI Studio (Build)</a> can build and test apps from ideas with advanced multimodal capabilities from Gemini, and <a href=https://manus.im/>Manus</a> orchestrates a huge number of subagents and services like <a href=https://browser-use.com/>browser-use</a> to max out agent action space.
I&rsquo;ll leave the rest of the exploration to readers due to limited time.</p><h2 id=agentic-rl>Agentic RL<a hidden class=anchor aria-hidden=true href=#agentic-rl>#</a></h2><p><em>Agent</em>, <em>Action (tools)</em> and <em>Environment (scaffold)</em> are three tightly bound concepts in the traditional literature. <strong>Consistency between training (rollout) Gym and inference scaffold is critical to maintain agent performance.</strong> However, this consistency has become a luxury since model providers won&rsquo;t expose training infra, usually resulting in tedious engineering efforts guessing the &ldquo;right&rdquo; scaffolding and orchestrations among downstream agent builders.</p><p>Most LLMs are trained with vanilla sandbox scaffolds to support evaluation like <a href=https://www.vals.ai/benchmarks/terminal-bench>Terminal-Bench</a>, yet a language model doesn&rsquo;t natively &ldquo;know&rdquo; how to use your sandbox &ndash; especially when you want to customize the tools and &ldquo;Skills&rdquo;. In this case, RL becomes an effective data-efficient method for your last mile.</p><h3 id=rollout-infra>Rollout Infra<a hidden class=anchor aria-hidden=true href=#rollout-infra>#</a></h3><p>One key challenge in Agentic RL is to build stable and efficient rollout infra. The additional factor of sandbox and tools like browser impose difficulty in asynchronous runtime efficiency, state management, and security. These rollouts are usually <strong>magnitudes</strong> more expensive (time and effort) than non-agentic RL like Math CoT. A nice starting point is <a href=https://arxiv.org/pdf/2511.03690v1>OpenHands V1</a> released lately. The architecture of decoupled modules (abstraction, tools, sandbox, and server) provides solid coordination with reusable modules across scaffolding and rollout serving. Besides, <a href=https://github.com/meta-pytorch/OpenEnv>Pytorch OpenEnv</a> provides a nice Gymnasium-style endpoint over commonly used agent docker environments.</p><figure class=align-center><img loading=lazy src=./assets/openhandsv1.png#center alt="OpenHands V1 architecture" width=500><figcaption><p><em>OpenHands V1 with decoupled modules reusable across rollout and scaffolding.</em></p></figcaption></figure><h3 id=reward-design-and-training-recipe>Reward Design and Training Recipe<a hidden class=anchor aria-hidden=true href=#reward-design-and-training-recipe>#</a></h3><p>Another challenge in Agentic RL is defining the reward function. Computer-use agents usually tackle open-ended tasks like research and app building, which lack <strong>unbiased</strong> and <strong>verifiable</strong> scoring mechanisms as in <em>Math</em> and <em>Coding</em>. Meanwhile, vanilla use of LLM-as-judge to generate rewards can easily trap the policy into <strong>adversarial distribution</strong> from the teacher (Reward Hacking). This is also confirmed in Andrej&rsquo;s recent <a href="https://x.com/dwarkesh_sp/status/1979234976777539987?s=20">interview</a> &ldquo;RL is terrible&rdquo;. An interesting approach to mitigate reward hacking in open-ended research is brought by AI2 <a href=https://allenai.org/blog/dr-tulu>DR Tulu</a>, where rubrics are buffered and generated on the fly together with policy. The algorithm setups used for Agentic RL generally follow the lessons from long-context RL in LLMs, such as <a href=https://arxiv.org/pdf/2510.01180>broadening exploration</a> in <a href=https://arxiv.org/pdf/2505.24864>prolonged</a> steps.</p><figure class=align-center><img loading=lazy src=./assets/dr_tulu.png#center alt="DR Tulu RLER" width=500><figcaption><p><em>Reinforcement Learning with Evolving Rubrics (RLER)</em></p></figcaption></figure><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>This post showcases the state of Computer-use Agents &ndash; from product to design principles. It elaborates the reasoning and importance of runtime sandbox in AI Agents and briefly introduces the engineering and training challenges. <strong>2026 will be the year of Computer-use Agents &ndash; with focus shifting from <del>Prompt Engineering</del> to Sandbox Engineering and RL on custom scaffolds</strong> ;)</p><hr><h3 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>Xu, Binfeng. &#34;Demystifying Agent Sandbox&#34;. B&#39;Log (Dec 2025). https://billxbf.github.io/posts/demystify-agent-sandbox/
</span></span></code></pre></div></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/hello_world/><span class=title>Next »</span><br><span>Why I start to write</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>B'Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");if(menu){const e=localStorage.getItem("menu-scroll-position");e&&(menu.scrollLeft=parseInt(e,10)),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}}document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{const e=document.querySelector("html");e.dataset.theme==="dark"?(e.dataset.theme="light",localStorage.setItem("pref-theme","light")):(e.dataset.theme="dark",localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>